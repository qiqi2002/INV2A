{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/env1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]\n",
      "/root/miniconda3/envs/env1/lib/python3.11/site-packages/models/inversion_model.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from models.inversion_model import InvEncoder, InvDecoder, InvEncoderDecoder, Projector, get_hidden_size\n",
    "\n",
    "encoder_model_path = '/root/autodl-tmp/inv-general/InvEncoder'\n",
    "decoder_model_path = '/root/autodl-tmp/sft-full-mix_50k'\n",
    "projector_model_path = '/root/autodl-tmp/inv-general/projector.pt'\n",
    "encoder = InvEncoder(model_path=encoder_model_path)\n",
    "decoder = InvDecoder(model_path=decoder_model_path)\n",
    "\n",
    "encoder_hidden_size = get_hidden_size('t5-base')\n",
    "decoder_hidden_size = get_hidden_size('llama2')\n",
    "projector = Projector(\n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=decoder_hidden_size,\n",
    "    model_path=projector_model_path\n",
    ")\n",
    "inv_model = InvEncoderDecoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    projector=projector,\n",
    ")\n",
    "inv_model.to('cuda')\n",
    "print('model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_subwords_to_word(tokens):\n",
    "    mapping = []\n",
    "    word_idx = 0\n",
    "    word_now = ''\n",
    "    words = []\n",
    "    last_token = ''\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"▁\") or token in ['<s>', '</s>', '!', ':', '?', '<0x0A>'] or token.startswith('.') or last_token==']' or last_token=='<0x0A>' or token.startswith(')'):\n",
    "            word_idx += 1\n",
    "            words.append(word_now)\n",
    "            word_now = ''\n",
    "        mapping.append(word_idx)\n",
    "        word_now += token\n",
    "        last_token = token\n",
    "    words.append(word_now)\n",
    "    return words[1:], [_-1 for _ in mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/root/miniconda3/envs/env1/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/env1/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 processing...\n",
      "t2 processing...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from visualizer import Visualizer\n",
    "\n",
    "def inversion_template(x):\n",
    "    return '[inversion]' + x + '[/inversion]'\n",
    "\n",
    "words_list = []\n",
    "weights_list = []\n",
    "\n",
    "x_label = 'How do you feel about the current political climate in the us? what is one thing that you would like to change? Output:'\n",
    "y = ' I feel very negatively about the current political climate in the US. I would like to see more bipartisanship and less division between Democrats and Republicans.'\n",
    "\n",
    "template_y = inversion_template(y)\n",
    "\n",
    "t1 = AutoTokenizer.from_pretrained(encoder_model_path)\n",
    "t2 = AutoTokenizer.from_pretrained(decoder_model_path)\n",
    "viz = Visualizer(decoder.model, t2)\n",
    "\n",
    "res = t1(template_y, return_tensors='pt', padding=True, truncation=True)\n",
    "encoder_input_len = res['input_ids'].shape[1]\n",
    "\n",
    "# for p in ['', template_y]:\n",
    "for p in [template_y]:\n",
    "    res = t1(p, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoder_input_ids = res['input_ids'].to('cuda')\n",
    "    encoder_input_attention_mask = res['attention_mask'].to('cuda')\n",
    "    # if p=='':\n",
    "        # encoder_input_ids = torch.concat([encoder_input_ids, torch.full((1, encoder_input_len-1), 1).to('cuda')], dim=1)\n",
    "        # encoder_input_ids = torch.concat([encoder_input_ids, torch.full((1, encoder_input_len-1), t1.pad_token_id).to('cuda')], dim=1)\n",
    "        # encoder_input_attention_mask = torch.concat([encoder_input_attention_mask, torch.full((1, encoder_input_len-1), 1).to('cuda')], dim=1)\n",
    "    \n",
    "    hidden_states, hidden_attention_mask = inv_model.forward_hidden_states(\n",
    "        encoder_input_ids=encoder_input_ids,\n",
    "        encoder_input_attention_mask=encoder_input_attention_mask,\n",
    "    )\n",
    "    encoder_embeds = hidden_states\n",
    "    res = t2(template_y, return_tensors='pt')\n",
    "\n",
    "    decoder_input_ids = res['input_ids'].to('cuda')\n",
    "    decoder_attention_mask = res['attention_mask'].to('cuda')\n",
    "    text_embeds = decoder.embed_input_ids(decoder_input_ids)\n",
    "\n",
    "\n",
    "    merge_embeds = torch.concat([encoder_embeds, text_embeds], dim=1)\n",
    "    merge_attention = torch.concat([hidden_attention_mask, decoder_attention_mask], dim=1)\n",
    "\n",
    "    outputs = decoder.model.generate(inputs_embeds=merge_embeds, attention_mask=merge_attention, labels=None, do_sample=False)\n",
    "\n",
    "    token_grads = viz.vis_by_grad_embeds(merge_embeds, x_label)\n",
    "    \n",
    "    print('t1 processing...')\n",
    "    res = t1(p, return_tensors='pt')\n",
    "    # if p=='':\n",
    "    #     encoder_input_ids = res['input_ids'].to('cuda')\n",
    "    #     encoder_input_attention_mask = res['attention_mask'].to('cuda')\n",
    "        # encoder_input_ids = torch.concat([encoder_input_ids, torch.full((1, encoder_input_len-1), 1).to('cuda')], dim=1)\n",
    "        # encoder_input_ids = torch.concat([encoder_input_ids, torch.full((1, encoder_input_len-1), t1.pad_token_id).to('cuda')], dim=1)\n",
    "        # # encoder_input_attention_mask = torch.concat([encoder_input_attention_mask, torch.full((1, encoder_input_len-1), 1).to('cuda')], dim=1)\n",
    "        # res['input_ids'] = encoder_input_ids\n",
    "        # res['attention_mask'] = encoder_input_attention_mask\n",
    "    len1 = res['input_ids'].shape[1]\n",
    "    decode_y = t1.batch_decode(res['input_ids'])[0]\n",
    "    t1_tokens = t1.tokenize(decode_y)\n",
    "    t1_token_grads = token_grads[:len1, :]\n",
    "\n",
    "    print('t2 processing...')\n",
    "    res = t2(template_y, return_tensors='pt')\n",
    "    len2 = res['input_ids'].shape[1]\n",
    "    t2_tokens = t2.tokenize('<s>'+template_y)\n",
    "    t2_token_grads = token_grads[len1:len1+len2, :]\n",
    "\n",
    "    tokens = t1_tokens + t2_tokens\n",
    "    names=tokens\n",
    "    token_grads = torch.concat([t1_token_grads, t2_token_grads], dim=0)\n",
    "    values = [grad.norm().item() for grad in token_grads]\n",
    "\n",
    "    words, mapping = map_subwords_to_word(tokens)\n",
    "\n",
    "    word_grads = [torch.zeros_like(token_grads[0]) for _ in range(len(words))]  # Initialize gradient vectors for each word\n",
    "    for idx, grad in enumerate(token_grads):\n",
    "        word_grads[mapping[idx]] += grad\n",
    "    weights = [x.norm().item() for x in word_grads]\n",
    "\n",
    "    words_list.append(words)\n",
    "    weights_list.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39726948738098145, 0.4591701924800873, 0.1297350525856018, 0.18277420103549957, 0.1304171234369278, 0.24692991375923157, 0.15261857211589813, 0.10805866867303848, 0.1125379279255867, 0.11912064254283905, 0.1737215220928192, 0.08462700247764587, 0.13007865846157074, 0.25984832644462585, 2.2333953380584717, 0.155464306473732, 0.18522672355175018, 0.13064514100551605, 0.0949675515294075, 0.07916409522294998, 0.07739176601171494, 0.22385768592357635, 0.07543805241584778, 0.060687460005283356, 0.06442674249410629, 0.06355589628219604, 0.17505645751953125, 0.06109282374382019, 0.0670047327876091, 0.4708525538444519]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "for i in range(1):\n",
    "    start_idx = words_list[i].index('<s>')\n",
    "    words_list[i] = words_list[i][start_idx:]\n",
    "    weights_list[i] = weights_list[i][start_idx:]\n",
    "    words_list[i] = [x.replace('▁','') for x in words_list[i]]\n",
    "    print(weights_list[i])\n",
    "\n",
    "from html_template import html_template\n",
    "s = html_template.format(\n",
    "    DATA1=json.dumps({\n",
    "        'words': words_list[0],\n",
    "        'weights': weights_list[0],\n",
    "    })\n",
    ")\n",
    "with open('viz.html', 'w') as f:\n",
    "    f.write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
